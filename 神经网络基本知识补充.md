# 神经网络基本知识补充

神经元：输入经过线性函数计算，和非线性函数的激活/处理，最后输出结果。

神经网络由大量神经元的有向连接构成。

> 1. 拓扑结构，即不同神经元之间的连接关系。
> 2. 激活规则，即输入到输出的映射关系（非线性的激活函数）。
> 3. 学习算法，通过训练来学习网络里主要参数。

## 1. 根据拓扑结构分类

前向网络，反向网络，图网络

> 1. 前向网络，神经元分组，每组视为一层神经元，接收上一层的输出作为输入，信息传播是单向的，可以用有向无环图表示。
> 2. 反向/反馈网络，每个神经元在接收上层神经元反馈信号时，也可以接收来自自身的反馈信号，如RNN。
> 3. 图网络，每个神经元看做是节点，每个节点只接收临近节点和自身的反馈信号，如GNN、消息传递网络。

## 2. 感知机

神经元结构的实例，用来接收多个信号，输出一个信号（由多个神经元组成），每个输入具有一定的权重，计算多个输入信号的值与权重的乘积和，根据结果与指定阈值的比较，来决定该神经元是否被激活。

### 2.1 感知机-算法公式

核心的线性函数部分，一般时求乘积和，其后跟一个激活函数，进行分类（找到分类超平面）。

误分类点越少，误分类点距离超平面越近，损失函数值越小。

### 2.2 感知机-损失函数

计算超平面的过程需要定义损失函数，即期望使“误分类样本到超平面距离之和”最小。

### 2.3 感知机-梯度下降优化

使用方法：随机梯度下降（SGD），小批量梯度下降（MBGD）

若使用SGD，则感知机每次需要使用一个误分类点来更新梯度，更新原则是：若预测准确，则不更新权重；否则增加权重，使其更趋向于正确的类别。

#### 更新步骤：

- 初始化，选择Θ向量的初值和步长α的初值
- 训练集中选择一个误分类点，计算损失函数结果，判断是否需要更新权重
- 对Θ向量进行一次随机梯度下降迭代
- 检查是否还有误分类点，重复2，3步骤
	- 若两个类别线性可分，则感知器会收敛，初始值不会影响收敛；若线性不可分，则感知器一定不会收敛

## 3. 激活函数

- sigmoid 函数，大约5层左右就会出现梯度饱和，难以训练，即梯度消失；
- tanh 函数，更接近自然梯度，可看做放大的logist回归函数；
- relu 函数，整流线性函数，修正的线性单元（非连续、不平滑的），收敛快速，但可能导致某些神经元成为死亡神经元；
- leaky relu 渗漏整流线性单元，给所有负值赋予一个非零斜率，防止死亡Relu的出现；
- softmax 函数，将一个k维实数向量映射到另一个k维实数向量，每个元素都在(0,1)之间，新向量所有维度模长之和为1；常用于多分类任务的输出层（将0-1之间分成多少区间，即能实现多少种分类结果）；

> 激活函数的设计：
> > 非线性  
> > 连续可微性  
> > 有界性  
> > 单调性  
> > 平滑性  

## 4. 前馈神经网络

- 神经网络在感知器的基础上做了哪些扩展：

> 加入隐藏层，隐藏层可以有多层，增强模型的表达能力
> 输出层的神经元可以不止一个，可以有多个输出，可灵活应用于分类、回归、降维、聚类等
> 对激活函数的扩展，Sigmoid、Softmax、Relu等

- 隐藏层大于2时，称为深度网络DNN
- 深度前馈网络DFN，也叫前馈神经网络FNN，或者多层感知机MLP
- 输入层（第0层）提供输入值，隐藏层包含多层感知机，每层都包含了线性函数+激活函数
- 表达式推导，参见：https://education.huaweicloud.com/courses/course-v1:HuaweiX+CBUCNXE174+Self-paced/courseware/9a522738e65140d8bea6ec80dae38e22/9695cd546e184727a725f133f0fc0c2d/?child=first

## 5. 反向传播-更新网络中的权重和偏置

训练过程：
- 正向传播，y1=f1(w1x1 + w2x2)
- 误差反向传播，根据最终的损失值，计算最后一层的δ，反向计算倒数第二层的δ，直到计算出第一层的δ；
- 权重更新，权重的增加值=学习率*δ1*偏导*x1
- 具体讲解参见：https://education.huaweicloud.com/courses/course-v1:HuaweiX+CBUCNXE174+Self-paced/courseware/bc524a77fb0c43cda70053918197aca8/e6bac657d5014ec1a07b70ab29afff06/?child=first


- 梯度消失：激活函数求导后与权重乘积小于1，随着链式求导层数越多，梯度更新信息会以指数形式衰减，导致梯度消失；
- 梯度爆炸，激活函数求导后与权重乘积大于1，梯度更新信息会以指数形式增加，梯度较大会导致网络性能不稳定；


## 6. 神经网络架构设计-深度和参数数量对网络性能的影响

- 网络深度，随着网络层数的增加，网络性能如准确率会有提升，但不是线性提升的，可能出现对数增加趋势，即到了某些节点，继续增加层数不会大幅增加准确度，反而要消耗更多的计算资源
- 参数数量，层数相同时，参数数量对结果的影响不大；层数增加会大幅度增加参数数量
